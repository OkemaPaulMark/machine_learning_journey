{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reinforment Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e1cbebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement Learning (RL)\n",
    "# What is RL?\n",
    "# Reinforcement Learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward. The agent interacts with the environment, receives feedback in the form of rewards or penalties, and adjusts its actions accordingly.\n",
    "# Key Concepts:\n",
    "# 1. Agent: The learner or decision-maker.\n",
    "# 2. Environment: The context in which the agent operates.\n",
    "# 3. State: A representation of the current situation of the agent in the environment.\n",
    "# 4. Action: A decision made by the agent that affects the state of the environment.\n",
    "# 5. Reward: Feedback from the environment based on the action taken by the agent.\n",
    "# 6. Policy: A strategy that the agent employs to determine its actions based on the current state.\n",
    "# 7. Value Function: A function that estimates the expected cumulative reward for a given state or state-action pair.\n",
    "\n",
    "# Key characteristics of RL:\n",
    "# No labelled data: Unlike supervised learning, RL does not require labelled data. The agent learns from the consequences of its actions.\n",
    "# Focus on learning from interaction: The agent learns by interacting with the environment, exploring different actions, and observing the outcomes.\n",
    "# Involves exploration and exploitation: The agent must balance exploring new actions to discover their effects and exploiting known actions that yield high rewards.\n",
    "# Working throught through delay rewards: The agent may receive rewards after a series of actions, requiring it to learn to associate actions with long-term outcomes.\n",
    "\n",
    "# Reinforcment Learning Algorithms:\n",
    "# 1. Q-Learning: A model-free algorithm that learns the value of actions in a given state, allowing the agent to choose actions that maximize cumulative reward.\n",
    "# 2. Deep Q-Networks (DQN): An extension of Q-learning that uses deep neural networks to approximate the Q-value function, enabling the agent to handle high-dimensional state spaces.\n",
    "# 3. Policy Gradient Methods: These methods directly optimize the policy by adjusting the parameters of the policy network based on the received rewards.\n",
    "# 4. Actor-Critic Methods: These methods combine value-based and policy-based approaches, using an actor to select actions and a critic to evaluate the actions taken.\n",
    "# 5. Proximal Policy Optimization (PPO): A popular policy gradient method that uses a surrogate objective function to update the policy, ensuring stable and efficient learning.\n",
    "# 6. Trust Region Policy Optimization (TRPO): A policy optimization method that constrains the policy update to ensure stability and prevent large changes in the policy.\n",
    "# 7. Monte Carlo Methods: These methods estimate the value of states or actions based on the average rewards received from multiple episodes.\n",
    "\n",
    "# Q-Learning:\n",
    "# Environment  (Position, Goal, Reward)\n",
    "\n",
    "# Action (Move Left, Move Right, Stay)\n",
    "# State (Current Position)\n",
    "# Reward (Positive for reaching the goal, Negative for hitting a wall)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762d39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a simple Q-learning agent that learns to navigate a grid environment to reach a goal while avoiding walls.\n",
    "# # Import necessary libraries\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe417ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment parameters\n",
    "position = 5  # Initial position of the agent 0 to 4\n",
    "actions = 2  # 0: Move Left, 1: Move Right\n",
    "\n",
    "# Initialize Q-table\n",
    "# Create a Q-table with states (positions) and actions (left/right)\n",
    "Q = np.zeros((position, actions))\n",
    "\n",
    "# Define parameters\n",
    "episodes = 1000  # Number of episodes for training\n",
    "learning_rate = 0.8  # Learning rate\n",
    "gamma = 0.9  # Discount factor for future rewards\n",
    "epislon = 0.3  # Probability of taking a random action (exploration)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    # Randomly select an initial state (position)\n",
    "    state = random.randint(0, position - 1)\n",
    "\n",
    "# Action selection (Epsilon-greedy policy)\n",
    "    if random.uniform(0, 1) < epislon:  # Explore with probability epislon\n",
    "        action = random.randint(0, actions - 1)  # Randomly select an action\n",
    "    else:  # Move right\n",
    "        action = np.argmax(Q[state])  # Exploit the best known action\n",
    "\n",
    "# Take action\n",
    "    # Simulate environment response (for simplicity, assume moving left or right)\n",
    "    if action == 0:  # Move Left\n",
    "        # Ensure the position does not go below 0\n",
    "        next_state = max(0, state - 1)\n",
    "    else:  # Move Right\n",
    "        # Ensure the position does not exceed the maximum\n",
    "        next_state = min(position - 1, state + 1)\n",
    "\n",
    "    # Reward structure (for simplicity, assume reaching position 4 is the goal)\n",
    "    if next_state == position - 1:  # If reached the goal\n",
    "        reward = 10  # Positive reward for reaching the goal\n",
    "    else:\n",
    "        reward = -1  # Negative reward for each step taken\n",
    "\n",
    "    # Q-learning update\n",
    "    # Update Q-value using the Q-learning formula\n",
    "    Q[state, action] += learning_rate * \\\n",
    "        (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "\n",
    "    # Transition to the next state\n",
    "    state = next_state  # Update the current state to the next state\n",
    "\n",
    "    # Show Learned Q-table\n",
    "    print(\"Q-table:\")\n",
    "    print(Q)\n",
    "\n",
    "    # Test the train agent\n",
    "    state = 0  # Start from the initial position\n",
    "    steps = 0  # Initialize step counter\n",
    "    print(\"\\n Agent path goal:\")\n",
    "\n",
    "    # Agent#s path to the goal\n",
    "    while state < position - 1:  # While not reached the goal\n",
    "        action = np.argmax(Q[state])  # Select the best action based on Q-table\n",
    "        if action == 0:  # Move Left\n",
    "            # Ensure the position does not go below 0\n",
    "            next_state = max(0, state - 1)\n",
    "        else:  # Move Right\n",
    "            # Ensure the position does not exceed the maximum\n",
    "            next_state = min(position - 1, state + 1)\n",
    "\n",
    "        print(\n",
    "            f\"Step {steps}: Position {state} -> Action {action} -> Next Position {next_state}\")\n",
    "        state = next_state  # Update the current state to the next state\n",
    "        steps += 1  # Increment step counter\n",
    "    print(f\"Reached goal in {steps} steps.\")\n",
    "\n",
    "    # Final Q-table\n",
    "    print(\"\\nFinal Q-table:\")\n",
    "    print(Q)\n",
    "\n",
    "    # Assignment One train RL agent to navigate to cross the road with action right, left, right"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
